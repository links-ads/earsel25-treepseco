# @package _global_

# specify here default configuration
# order of defaults determines the order in which configs override each other
defaults:
  - model: point_decoder_finetune
  - data: ??? # noeon_tree or noasca_vda
  - training: point_decoder_finetune
  - _self_

unique_out_name: ???
save_root: outputs/cached_proposals/${unique_out_name}

hydra:
  run:
    dir: ${hydra:runtime.cwd}/${save_root}
  sweep:
    dir: ${hydra:runtime.cwd}/${save_root}

seed: 99
val_only: false

model:
  point_decoder:
    mask_upscaling_type: default # (transposed) or bilinear
    point_decoder_state_dict_path: pretrained/point_decoder/pt_dec_neon_pretrain_loss_04006.pth
    prefix_to_rmv_in_state_dict: "pretrain_sam_point_decoder."

training:
  loss_type: ssim # or mse